{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "34f74039-89b4-4b3a-b8e6-a876f7057509",
   "metadata": {},
   "source": [
    "# Step 1: Creating Tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "99f53552-3e5e-44e8-93e4-3bcb42683871",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total number of characters:  20479\n",
      "I HAD always thought Jack Gisburn rather a cheap genius--though a good fellow enough--so it was no \n"
     ]
    }
   ],
   "source": [
    "with open(\"the-verdict.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    raw_text = f.read() #storing the content form verdict.txt file\n",
    "\n",
    "print(\"total number of characters: \", len(raw_text))\n",
    "print(raw_text[:99]) #prints the first 100 characters of the file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c151902-c9cb-4d4b-9da3-f5eb93eaeaf0",
   "metadata": {},
   "source": [
    "our goal is to convert the 20479 characters into individual words and that we can turn into embeddigs for LLM training.\n",
    "\n",
    "\n",
    "now the question is how can we best split this text to obtain a list of tokens?\n",
    "for this we will use python's regular expression library and then split the text based on the white space or punctuations into individual tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "a82eb761-8847-4b4f-9362-232c3484ddc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello,', ' ', 'world.', ' ', 'This,', ' ', 'is', ' ', 'a', ' ', 'test.']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "text = \"Hello, world. This, is a test.\"\n",
    "result= re.split(r'(\\s)', text) #splits wherever whitespaces are encountered.\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc21f6c8-e2f0-48d8-ba2d-475151f4aa8d",
   "metadata": {},
   "source": [
    "from the above code we can see that the result is a list of individual words, whitespaces, and punctuations. Now lets modify the regular expression such that it splits on whitespaces (\\s) and commas, and period."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "00bf4cf1-4d6e-44a1-a4be-69cf46857f7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', ',', '', ' ', 'world', '.', '', ' ', 'This', ',', '', ' ', 'is', ' ', 'a', ' ', 'test', '.', '']\n"
     ]
    }
   ],
   "source": [
    "result = re.split(r'([,.]|\\s)', text)\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d4b4532-dcf9-46c0-86ef-da8313d58320",
   "metadata": {},
   "source": [
    " a small issue that we encounter here is that the list still includes the whitespace characters. we have to remove them , which is as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "76d1e2aa-7f43-4c26-bef7-74976bea32c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', ',', 'world', '.', 'This', ',', 'is', 'a', 'test', '.']\n"
     ]
    }
   ],
   "source": [
    "result = [item for item in result if item.strip()] #scans each item in the result and removes whitespace\n",
    "#item.strip() will only return true if there is a word or punctuation else return false and it will not print\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d05013df-d7f3-48d5-a43f-71093f005d6b",
   "metadata": {},
   "source": [
    "REMOVING WHITESPACES OR NOT?\n",
    "\n",
    "When developing a simple tokenizer , whether we should encode whitespaces as separate characters or just remove them depends on our application and its requirements. \n",
    "\n",
    "Advantages of removing white spaces is that it reduces the memory and computing requirements. However keeping them can be useful if we train models that are sensitive to the exact structure of the text. (for example , python code , which is sensitive to indentation and spacing)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfa98bed-1186-459c-b063-e7e4acccbba7",
   "metadata": {},
   "source": [
    "The tokenization scheme that we have used above is well enough but the input text can contain various other things such as question marks, quotation marks , double -dashes etc so we will again modify the splitting criteria based on the nature of this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "df7c254e-9db9-40e6-8cf1-d16d2199a08d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', ',', 'world', '!', '.', 'Is', 'this', '--', 'a', 'test', '?']\n"
     ]
    }
   ],
   "source": [
    "text = \"Hello, world!. Is this-- a test?\"\n",
    "result = re.split(r'([,.:;?_!\"()\\']|--|\\s)', text)\n",
    "\n",
    "result = [item.strip() for item in result if item.strip()]\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "18171ade-5855-4aeb-ad9a-33bd078e2860",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', ',', 'world', '!', '.', 'Is', 'this', '--', 'a', 'test', '?']\n"
     ]
    }
   ],
   "source": [
    "#strip whitespace from each item and then filter out any empty string\n",
    "\n",
    "result = [item for item in result if item.strip()]\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "eb2f8784-c64e-4138-b1d0-4a19742a172d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', ',', 'world', '!', '.', 'Is', 'this', '--', 'a', 'test', '?']\n"
     ]
    }
   ],
   "source": [
    "text = \"Hello, world!. Is this-- a test?\"\n",
    "result = re.split(r'([,.:;?_!\"()\\']|--|\\s)', text)\n",
    "\n",
    "result = [item.strip() for item in result if item.strip()] \n",
    "#the first item.strip() checks for whitespaces at the beginning of the sentence.\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9b95d4b-004f-47d5-908e-9815b8d2ba76",
   "metadata": {},
   "source": [
    "### Now let's apply this tokenizer to the our raw data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c39c0a5d-f9a5-4919-8bc0-9b15e70300b4",
   "metadata": {},
   "source": [
    "apply this tokenizer to or data and then store it to a variable named preprocessed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "a89a542c-ee04-40f7-b09f-f47118727981",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'HAD', 'always', 'thought', 'Jack', 'Gisburn', 'rather', 'a', 'cheap', 'genius', '--', 'though', 'a', 'good', 'fellow', 'enough', '--', 'so', 'it', 'was', 'no', 'great', 'surprise', 'to', 'me', 'to', 'hear', 'that', ',', 'in']\n"
     ]
    }
   ],
   "source": [
    "preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', raw_text)\n",
    "preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
    "print(preprocessed[:30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "20ea2de9-f0f0-418a-83d8-8bbed8068023",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4690\n"
     ]
    }
   ],
   "source": [
    "print(len(preprocessed)) #this prints the length of the entire preprocessed token."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c467d0a9-3fb1-4e6c-a64d-853c96cb61ae",
   "metadata": {},
   "source": [
    "we have successully tokenized the entire dataset that we had and now we proceed to the second step where we assign ids to the tokens because machines cannot understand the tokens directly we have to assign IDs to the tokens."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "693c2de0-bc1d-4541-a218-68cfa1e51f9d",
   "metadata": {},
   "source": [
    "# Step 2: Creating Token IDs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43fc589e-b9a9-46e3-a795-fb65db227af8",
   "metadata": {},
   "source": [
    "in this step we will sort the tokens in the preprocessed variable in alphabetical order and then determine the vocabulary size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "6d48353b-2512-45f4-b633-741aece919a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1130\n"
     ]
    }
   ],
   "source": [
    "all_words = sorted(set(preprocessed))  #converting it into a set and then sorting in the set\n",
    "vocab_size = len(all_words)\n",
    "\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56b55251-2da2-459c-a600-9e9fdd8f5f67",
   "metadata": {},
   "source": [
    "here the number is less as compared to the tokens because the vocab size is the count of only the unique toekns that are present in the preprocessed variable.\n",
    "\n",
    "\n",
    "now assigning this to vocabulary where vocabulary is like a dictionary of tokens and their associated token IDs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "20ec07da-a530-48f0-b475-03519f5517b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = {token:integer for integer, token in enumerate(all_words)}\n",
    "#this will assign integer to each and every unique token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "7f9efbaa-b708-4366-903e-7e72a4d16f5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('!', 0)\n",
      "('\"', 1)\n",
      "(\"'\", 2)\n",
      "('(', 3)\n",
      "(')', 4)\n",
      "(',', 5)\n",
      "('--', 6)\n",
      "('.', 7)\n",
      "(':', 8)\n",
      "(';', 9)\n",
      "('?', 10)\n",
      "('A', 11)\n",
      "('Ah', 12)\n",
      "('Among', 13)\n",
      "('And', 14)\n",
      "('Are', 15)\n",
      "('Arrt', 16)\n",
      "('As', 17)\n",
      "('At', 18)\n",
      "('Be', 19)\n",
      "('Begin', 20)\n",
      "('Burlington', 21)\n",
      "('But', 22)\n",
      "('By', 23)\n",
      "('Carlo', 24)\n",
      "('Chicago', 25)\n",
      "('Claude', 26)\n",
      "('Come', 27)\n",
      "('Croft', 28)\n",
      "('Destroyed', 29)\n",
      "('Devonshire', 30)\n",
      "('Don', 31)\n",
      "('Dubarry', 32)\n",
      "('Emperors', 33)\n",
      "('Florence', 34)\n",
      "('For', 35)\n",
      "('Gallery', 36)\n",
      "('Gideon', 37)\n",
      "('Gisburn', 38)\n",
      "('Gisburns', 39)\n",
      "('Grafton', 40)\n",
      "('Greek', 41)\n",
      "('Grindle', 42)\n",
      "('Grindles', 43)\n",
      "('HAD', 44)\n",
      "('Had', 45)\n",
      "('Hang', 46)\n",
      "('Has', 47)\n",
      "('He', 48)\n",
      "('Her', 49)\n",
      "('Hermia', 50)\n"
     ]
    }
   ],
   "source": [
    "for i, item in enumerate(vocab.items()):\n",
    "    print(item)\n",
    "    if i>=50:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "c5e2b689-4936-49aa-82e8-97494767fb0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleTokenizerV1:\n",
    "    def __init__(self, vocab):\n",
    "        self.str_to_int = vocab\n",
    "        self.int_to_str = {i:s for s, i in vocab.items()} #needed for the decoder part to convert num to token\n",
    "\n",
    "    def encode(self, text):\n",
    "        preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', text)\n",
    "        preprocessed = [\n",
    "            item.strip() for item in preprocessed if item.strip()\n",
    "        ]\n",
    "        ids = [self.str_to_int[s] for s in preprocessed] #converting tokens into token IDs.\n",
    "        return ids\n",
    "\n",
    "    def decode(self, ids):\n",
    "        text = \" \".join([self.int_to_str[i] for i in ids]) #using reverse dictionary to convert token IDs to tokens\n",
    "        #replace spaces before specified punctuations , so  that it makes a perfect sentence.\n",
    "        text = re.sub(r'\\s+([,.?!\"()\\'])', r'\\1', text)\n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "688c4212-edaa-4f30-aaa3-1c5835ab72ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 56, 2, 850, 988, 602, 533, 746, 5, 1126, 596, 5, 1, 67, 7, 38, 851, 1108, 754, 793, 7]\n"
     ]
    }
   ],
   "source": [
    "#trying th etokenizer class that we have created by taking a sub part of the dataset for testing\n",
    "tokenizer = SimpleTokenizerV1(vocab)\n",
    "\n",
    "text = \"\"\"\"It's the last he painted, you know,\"\n",
    "            Mrs. Gisburn said with pardonable pride.\"\"\"\n",
    "\n",
    "ids = tokenizer.encode(text)\n",
    "print(ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c570cdaf-c200-4ed2-9107-5c6cf5d4f3ea",
   "metadata": {},
   "source": [
    "our tokenizer has converted the tokens into token IDs now let's test our decoeder whethter it can convert the token IDs back to tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "66c0a50c-dfb0-4cf0-8120-20607b294f9e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\" It\\' s the last he painted, you know,\" Mrs. Gisburn said with pardonable pride.'"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63806a9c-7dea-48ab-9686-95ce3abc99a2",
   "metadata": {},
   "source": [
    "from the above result we can see that we have successfully converted the tokens to token IDs and tokenIDs back to tokens from the subset of the training data. Now let's move further with it. What if we provide it with a sentence which is not present in the dataset?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "ff254cfc-23fc-48a9-9d99-ed80b1872cb4",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'Hello'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[72], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m#testing it with some words which are not present in the already available dataset.\u001b[39;00m\n\u001b[0;32m      3\u001b[0m text \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHello , do you like tea?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m----> 4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m)\n",
      "Cell \u001b[1;32mIn[69], line 11\u001b[0m, in \u001b[0;36mSimpleTokenizerV1.encode\u001b[1;34m(self, text)\u001b[0m\n\u001b[0;32m      7\u001b[0m preprocessed \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m([,.:;?_!\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m()\u001b[39m\u001b[38;5;130;01m\\'\u001b[39;00m\u001b[38;5;124m]|--|\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124ms)\u001b[39m\u001b[38;5;124m'\u001b[39m, text)\n\u001b[0;32m      8\u001b[0m preprocessed \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m      9\u001b[0m     item\u001b[38;5;241m.\u001b[39mstrip() \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m preprocessed \u001b[38;5;28;01mif\u001b[39;00m item\u001b[38;5;241m.\u001b[39mstrip()\n\u001b[0;32m     10\u001b[0m ]\n\u001b[1;32m---> 11\u001b[0m ids \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstr_to_int\u001b[49m\u001b[43m[\u001b[49m\u001b[43ms\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m preprocessed] \u001b[38;5;66;03m#converting tokens into token IDs.\u001b[39;00m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ids\n",
      "\u001b[1;31mKeyError\u001b[0m: 'Hello'"
     ]
    }
   ],
   "source": [
    "#testing it with some words which are not present in the already available dataset.\n",
    "\n",
    "text = \"Hello , do you like tea?\"\n",
    "print(tokenizer.encode(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31a00042-4679-4934-842a-df78c879ad00",
   "metadata": {},
   "source": [
    "we get an error for the above sentence because we don't have the word Hello in our dataset and from this we get to know that we need to consider large and diverse training sets to exxtend the vocabulary when working on LLMs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f25c2d02-7254-4a90-8776-803622ddc1b3",
   "metadata": {},
   "source": [
    "# Adding Special Context Tokens\n",
    "\n",
    "in the previous section we have implemented a simple tokenizer and which when tested to tokenize a word which was not present in the trainig data it gave an error. So in this section we will modify the tokenizer to handle unknown words. I particular, we will modify the vocabulary and tokenizer we implemented in the previous section, here we will implement the version 2 of SimpleTokenizer to handle the unknown tokens."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce4f411e-024b-41e5-a4b1-e873cc6226df",
   "metadata": {},
   "source": [
    "we can modify the tokenizer to use an <|unk|> token if it encounters a word that is not a part of the voocabulary. Furhtermore we add a token between unrelated tasks. \n",
    "For example , when training GPT-like LLMs multiple independent documents or books, it is common to insert a token before each document or book that follows a previous text source.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f1c3eb9-72c9-4092-9644-1ef6d3797722",
   "metadata": {},
   "source": [
    "modifying the vocabulary to include these two special tokens , <|endoftext|> and <|unk|> to the existing vocabulary. Previously the size of the vocabulary was 1130 and after addding this two tokens it would increase and become 1132."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "ed21d709-45f7-4052-925d-75707ce3fc5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_tokens = sorted(list(set(preprocessed)))\n",
    "all_tokens.extend([\"<|endoftext|>\", \"<|unk|>\"])\n",
    "\n",
    "vocab = {token:integer for integer, token in enumerate(all_tokens)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "d6cf4112-d095-4411-a515-55f6180ae392",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1132"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "3d8af38e-6033-41b7-9041-09aef54a8e2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('younger', 1127)\n",
      "('your', 1128)\n",
      "('yourself', 1129)\n",
      "('<|endoftext|>', 1130)\n",
      "('<|unk|>', 1131)\n"
     ]
    }
   ],
   "source": [
    "#for checking our modification we are printing the last 5 entries of the vocabulary.\n",
    "for i, item in enumerate(list(vocab.items())[-5:]):\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "2e469ae8-4847-4c95-a1bd-7a80566b30ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "#now further we will extend the simple tokenizer class with this.\n",
    "\n",
    "class SimpleTokenizerV2:\n",
    "    def __init__(self, vocab):\n",
    "        self.str_to_int = vocab\n",
    "        self.int_to_str = {i:s for s, i in vocab.items()}\n",
    "\n",
    "    def encode(self, text):\n",
    "        preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', text)\n",
    "        preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
    "        preprocessed = [\n",
    "            item if item in self.str_to_int\n",
    "            else \"<|unk|>\" for item in preprocessed #scans the entire text and if comes across unknown word \n",
    "        ]\n",
    "        ids = [self.str_to_int[s] for s in preprocessed]\n",
    "        return ids\n",
    "\n",
    "    def decode(self, ids):\n",
    "        text = \" \".join([self.int_to_str[i] for i in ids])\n",
    "        #replace spaces before the specified punctuations\n",
    "        text = re.sub(r'\\s+([,.:;?!\"()\\'])', r'\\1', text)\n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "c77be98a-d0da-49ec-b30b-565685db0f96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, do you like tea?<|endoftext|>In the sunlit terraces of the palace.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = SimpleTokenizerV2(vocab)\n",
    "\n",
    "text1= \"Hello, do you like tea?\"\n",
    "text2= \"In the sunlit terraces of the palace.\"\n",
    "\n",
    "text = \"<|endoftext|>\".join((text1, text2))\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "8965f1fb-91a9-4717-9dff-1f5b0d25bb41",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1131, 5, 355, 1126, 628, 975, 10, 1131, 988, 956, 984, 722, 988, 1131, 7]"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c026367-3172-4934-9c7b-0196de96443d",
   "metadata": {},
   "source": [
    "from the above results we can see that since hello was not present in our vocabulary it printed the token id of unk and for the endoftext also it did the same thing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "400f846a-3553-4141-8967-fec17c8fdfa4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|unk|>, do you like tea? <|unk|> the sunlit terraces of the <|unk|>.'"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(tokenizer.encode(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "892078b6-51ed-47c4-9ffe-bc08c3719238",
   "metadata": {},
   "source": [
    "based on the above detokenized words we can see that two words \"Hello\" and \"palace\" were not present in our vocabulary it were replaced with <|unk|> in the decoder part."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6ffd686-d75a-48fa-a659-273f29f60ee3",
   "metadata": {},
   "source": [
    "apart from this two tokens we also have other tokens which people use like \n",
    "1. [BOS] beginning of sequence - it marks the starting of the text. it signifies to the LLM where a piece of content begins.\n",
    "2. [EOS] end of sequence - it is positioned at the end of the text and is useful when concatenating multiple unrelated text.\n",
    "3. [PAD] padding - when training LLMs with batch sizes larger than one, the batch might contain texts of varying length . To ensure all texts have the same length the shorter texts are padded using the [PAD] token , up to the length of the longest text in the batch."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c1ffd84-9811-4b5a-9f83-cfb654580bff",
   "metadata": {},
   "source": [
    "# Byte-Pair Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e86329dd-5e0f-4b7c-88c3-2baa58080267",
   "metadata": {},
   "source": [
    "byte pair encoding is used in GPT-2 and GPT-3 and the original model.\n",
    "\n",
    "\n",
    "since implementing BPE is complicated we will be using an  existing python open-source library called tiktoken. OpenAI themselves use this library to convert the raw text into tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "7bf7af6f-ba5d-40f6-8e3e-a6a55f7971e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tiktoken version:  0.12.0\n"
     ]
    }
   ],
   "source": [
    "import importlib\n",
    "import tiktoken\n",
    "\n",
    "print(\"Tiktoken version: \", importlib.metadata.version(\"tiktoken\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "9b154c15-7930-4d80-9c43-3bca787eace1",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = tiktoken.get_encoding(\"gpt2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbabb43e-980f-4ff2-9745-6a8867fe45e1",
   "metadata": {},
   "source": [
    "the usage of the above tokenizer is similar to what we have developed in the above setion named the simpletoeknizerv1 and simpletokenizerv2 .  but here the entire thing is done in a single line of code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "c49f3220-df91-484a-b095-ca0fefcc9659",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15496, 11, 466, 345, 588, 8887, 30, 220, 50256, 554, 262, 4252, 77, 18250, 8812, 2114, 1659, 617, 34680, 27271, 13]\n"
     ]
    }
   ],
   "source": [
    "#lets see the working of this using a simple example\n",
    "text = (\n",
    "    \"Hello, do you like tea? <|endoftext|> In the sunnlit terraces\"\n",
    "    \"of someunknownPlace.\"\n",
    ")\n",
    "\n",
    "integers = tokenizer.encode(text, allowed_special={\"<|endoftext|>\"})\n",
    "\n",
    "print(integers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "c086ad82-0798-4881-91a8-76bdea1b471b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, do you like tea? <|endoftext|> In the sunnlit terracesof someunknownPlace.\n"
     ]
    }
   ],
   "source": [
    "#now we can convert this token ids back into words using the decode method .\n",
    "strings = tokenizer.decode(integers)\n",
    "print(strings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0909ae95-6073-43e1-8c16-fcba4883048d",
   "metadata": {},
   "source": [
    "# Creating Input-Target Pairs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b972e128-e54e-4ae5-b967-d9e373f36bf4",
   "metadata": {},
   "source": [
    "in this section we are going to implement a data loader that fetches the input-target pairs using a sliding window approach. \n",
    "\n",
    "to get started, we will first tokenize the whole The Verdict short story(the dataset we have been using for the earlier training.) we worked with earlier using the BPE tokenization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "66ac40bf-22da-4e8e-9e71-08d2d60206ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5145\n"
     ]
    }
   ],
   "source": [
    "with open(\"the-verdict.txt\", \"r\", encoding=\"utf-8\") as f: #reads the entire dataset\n",
    "    raw_text = f.read()  #stores it in variable named raw_text\n",
    "\n",
    "enc_text = tokenizer.encode(raw_text)\n",
    "print(len(enc_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09cbc54e-cac1-4808-bbb6-0b94d2ece91b",
   "metadata": {},
   "source": [
    "the above output shows that there are total of 5145 encoded tokens , voacbulary size is 5145 in the training set after applying the BPE tokenizer.\n",
    "\n",
    "now we will see the first 50 tokens from the dataset for demonstration as it results in a slightly more interesting text passage in the further steps. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "7854bc54-6ac7-4332-b7b6-34d47aed75a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_sample = enc_text[50:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cbd121f-2ff2-4351-9dea-f920c52a6550",
   "metadata": {},
   "source": [
    "Now one of the most easiest way to create input-target pairs for the nextword prediction task is to create two variables, x and y , where x contains input tokens and y contains the targets, which are the inputs shifted by 1: \n",
    "\n",
    "the context size determines how many tokens are included in the input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "01d1b7d8-719c-45ca-be3c-24214226d66b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: [290, 4920, 2241, 287]\n",
      "y:      [4920, 2241, 287, 257]\n"
     ]
    }
   ],
   "source": [
    "context_size = 4 #length of the input\n",
    "# The context_size of 4 menas that the model is trained to look at a sequence of \n",
    "# 4 words (or tokens ) to predict the next word in the sequence.\n",
    "# the input X is the first 4 tokens [1,2,3,4] and the target y is the next 4 tokens\n",
    "#[2,3,4,5]\n",
    "\n",
    "x = enc_sample[:context_size]\n",
    "y = enc_sample[1:context_size+1]\n",
    "\n",
    "print(f\"x: {x}\")\n",
    "print(f\"y:      {y}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5da04f66-fbd7-4595-8fe3-0106194053ec",
   "metadata": {},
   "source": [
    "processing the inputs along with the targets, which are the inputs shifted by one position, we can then create the next-word prediction tasks as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "4eb11a25-3539-4f10-a1b6-8ed607af3d16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[290] ----> 4920\n",
      "[290, 4920] ----> 2241\n",
      "[290, 4920, 2241] ----> 287\n",
      "[290, 4920, 2241, 287] ----> 257\n"
     ]
    }
   ],
   "source": [
    "for i in range(1, context_size+1):\n",
    "    context = enc_sample[:i]\n",
    "    desired = enc_sample[i]\n",
    "\n",
    "    print(context, \"---->\", desired)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54c318c2-8a6f-4b1d-880f-438a16120433",
   "metadata": {},
   "source": [
    "everything on the left of the arrow (---->) refers to the input an LLM would receive, and the token ID on the right side of the arrow represent what the LLM is supposed to predict."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "804a72e3-e9c3-4abf-8aea-e293c629da14",
   "metadata": {},
   "source": [
    "for demonstration purposes, let's repeat the previous code but convert the token IDs into text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "6d09ab29-b2dd-4b5f-b31c-1de3d7daf8b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " and ---->  established\n",
      " and established ---->  himself\n",
      " and established himself ---->  in\n",
      " and established himself in ---->  a\n"
     ]
    }
   ],
   "source": [
    "for i in range(1, context_size+1):\n",
    "    context = enc_sample[:i]\n",
    "    desired = enc_sample[i]\n",
    "\n",
    "    print(tokenizer.decode(context), \"---->\", tokenizer.decode([desired]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cd28f66-059b-4881-8c40-02bdaa025f31",
   "metadata": {},
   "source": [
    "we have now created input-target pairs that we can turn into use for the LLM training in the further chapters.\n",
    "\n",
    "now there's only one more task before we can turn the tokens into embeddings: implementing an efficient data loader that iterates over the dataset and returns the inputs and targets as PyTorch tensors, which can be thought of as multidimensional arrays.\n",
    "\n",
    "in particular, we are interested in returning two tensors: an input tensor containing the text that the LLM sees and a target tensor that includes the targets for the LLm to predict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c1307ff-a016-4718-86a1-757298ed2e17",
   "metadata": {},
   "source": [
    "# Implementing a Data Loader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08a490fe-2ca2-40a8-a259-8ed95c9e9fdc",
   "metadata": {},
   "source": [
    "for a efficient data loader implementation, we will use pytorch's built-in dataset and data Loader classes.\n",
    "\n",
    "Setps for the same :\n",
    "1. Tokenize the entire text\n",
    "2. use a sliding window to chunk the book into overlapping sequences of max_length\n",
    "3. return the total number of rows in the dataset\n",
    "4. return a single row from the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "00504ea7-8eb8-4c32-af26-52b8ef8d8ce6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class GPTDatasetV1(Dataset):\n",
    "    def __init__(self,txt, tokenizer, max_length,stride):\n",
    "        self.input_ids = []\n",
    "        self.target_ids = []\n",
    "\n",
    "        #tokenize the entire text\n",
    "        token_ids = tokenizer.encode(text, allowed_special = {\"<|endoftext|>\"})\n",
    "\n",
    "        #use a sliding window to chunk the book into overlapping sequences of max_length \n",
    "        for i in range(0, len(token_ids) - max_length, stride):\n",
    "            input_chunk = token_ids[i:i + max_length]\n",
    "            target_chunk = token_ids[i+1: i+ max_length + 1]\n",
    "            self.input_ids.append(torch.tensor(input_chunk))\n",
    "            self.target_ids.append(torch.tensor(target_chunk))\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.input_ids[idx], self.target_ids[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8bcb47c-8dee-45bd-8cce-9738cb34f271",
   "metadata": {},
   "source": [
    "the gptDatasetV1 class in listing 2.5 is based on the pytorch dataset class.\n",
    "it defines how individual rows are fetched from the dataset. Each row consists of a number of token IDs (based on max_length) assigned to an input_chunk tensor. \n",
    "the target_chunk tensor contains the corresponding targets.\n",
    "\n",
    "\n",
    "Since our data is ready we will feed this data into the data loader. we will write a code that will use the GPTDatasetV1 to load the inputs in batches via a PyTorch DataLoader.\n",
    "\n",
    "Below are the steps for the same:\n",
    "1. initialize the tokenizer\n",
    "2. create dataset\n",
    "3. drop_last = True drops the last bacth if it is shorter than the specified batch_size to prevent loss_spikes  during training.\n",
    "4. the number of CPU processes to use for preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "b5704c3c-20ef-4afa-afef-459aed8e4b2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataloader_v1(\n",
    "    txt,                    # raw training corpus as a single string\n",
    "    batch_size=4,            # how many sequences per batch\n",
    "    max_length=256,          # length (in tokens) of each training sample\n",
    "    stride=128,              # overlap between consecutive windows (controls data augmentation)\n",
    "    shuffle=True,            # shuffle batches each epoch (improves generalization)\n",
    "    drop_last=True,          # drop the last incomplete batch (keeps batch sizes uniform)\n",
    "    num_workers=0            # DataLoader worker processes (0 = do work in main process; safe on Windows/Jupyter)\n",
    "):\n",
    "    \"\"\"\n",
    "    Builds a PyTorch DataLoader for next-token prediction from a long text.\n",
    "    Splits `text` into overlapping token windows using `max_length` and `stride`.\n",
    "    \"\"\"\n",
    "\n",
    "    # 1) Initialize the tokenizer (GPT-2’s Byte-Pair Encoding).\n",
    "    #    This converts raw text into integer token IDs compatible with GPT-style models.\n",
    "    tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "    # 2) Create the dataset.\n",
    "    #    GPTDatasetV1 should: \n",
    "    #      - tokenize `txt`\n",
    "    #      - slice it into overlapping windows of length `max_length`\n",
    "    #      - with step size `max_length - stride`\n",
    "    #      - and return (input_ids, target_ids) pairs for next-token prediction.\n",
    "    dataset = GPTDatasetV1(txt, tokenizer, max_length, stride)\n",
    "\n",
    "    # 3) Wrap the dataset in a DataLoader to:\n",
    "    #      - batch samples (`batch_size`)\n",
    "    #      - optionally shuffle sample order each epoch (`shuffle`)\n",
    "    #      - optionally drop the last partial batch (`drop_last`)\n",
    "    #      - use `num_workers` background workers to speed up data loading\n",
    "    #    Note: On Windows/Jupyter, keep `num_workers=0` to avoid multiprocessing issues.\n",
    "    dataloader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=shuffle,\n",
    "        drop_last=drop_last,\n",
    "        num_workers=num_workers\n",
    "    )\n",
    "\n",
    "    # 4) Return an iterable that yields batches of tensors ready for training.\n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1977015f-ab05-441b-b53d-89436a640e7f",
   "metadata": {},
   "source": [
    "now we are going to test the dataloader with a batch size of 1 for an LLM with a context size of 4,\n",
    "this will develop an intuition of how the GPTDatasetV1 class and the create_dataloader_v1 function work together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "0a1a3d5b-bc38-4b1b-9f8d-bfdad3c2637d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#this is the first step where we read the text\n",
    "\n",
    "with open(\"the-verdict.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    raw_text = f.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6467c01e-7647-43cd-aea1-7dcc4637d1ab",
   "metadata": {},
   "source": [
    "converting the dataloader into a python iterator to fetch the next entry via python's built-in next() function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "4c7cd2bd-3fb3-40e5-a870-c1458fc9ffa6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pytorch version:  2.9.0+cpu\n",
      "[tensor([[15496,    11,   466,   345]]), tensor([[ 11, 466, 345, 588]])]\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "print(\"Pytorch version: \", torch.__version__)\n",
    "dataloader = create_dataloader_v1(\n",
    "    raw_text, batch_size=1, max_length=4, stride=1, shuffle=False\n",
    ")\n",
    "\n",
    "data_iter = iter(dataloader)\n",
    "first_batch = next(data_iter)\n",
    "print(first_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f7d58a4-624e-44c6-a800-4e1a7fe16276",
   "metadata": {},
   "source": [
    "here the first_batch variable contains two tensors: the first tensor stores the input token IDs, and the second tensor stores the target token IDs. Since the max_length is set to 4, each of the two tensors contains 4 token IDs.\n",
    "\n",
    "the input size of 4 is relatively small and only chosen for illustration purposes. it is common to train LLMs with input sizes of at least 256.\n",
    "\n",
    "\n",
    "to illustrate the menaing of stride=1, let's fetch another batch from this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "29355d43-5d90-4ad2-ad77-fe67607592e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[ 11, 466, 345, 588]]), tensor([[ 466,  345,  588, 8887]])]\n"
     ]
    }
   ],
   "source": [
    "second_batch = next(data_iter)\n",
    "print(second_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eb0065d-a9e3-45e1-8dd2-fe31351df911",
   "metadata": {},
   "source": [
    "it is called as sliding window approach because here we can see that during the second iteration the ones which were present in the output/ target tensor are now inputs during the second iteration."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce4e211d-dca1-44de-9e15-25d7d00cc907",
   "metadata": {},
   "source": [
    "batch sizes of 1, such as we have sampled from the data loader so far, are useful for illustration purposes. We know that in deep learning, a small batch sizes require less memory during training but lead to more noisy model updates.\n",
    "\n",
    "In regular deep learning, the batch size is a trade-off and hyperparameter to experiment with when training LLMs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3645a64-810d-4b9a-8cb0-d3cc853a96c9",
   "metadata": {},
   "source": [
    "before moving on to the two final sections of this chapter that we focused on creating embedding vectors from the tokenIDs , let's let us see how we can use the data loader to sample with a batch size greater than 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "2d4ee925-ec87-4fdf-aba5-67ff6769e0e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inputs:\n",
      " tensor([[15496,    11,   466,   345],\n",
      "        [  588,  8887,    30,   220],\n",
      "        [50256,   554,   262,  4252],\n",
      "        [   77, 18250,  8812,  2114],\n",
      "        [ 1659,   617, 34680, 27271]])\n",
      "\n",
      "Targets:\n",
      " tensor([[   11,   466,   345,   588],\n",
      "        [ 8887,    30,   220, 50256],\n",
      "        [  554,   262,  4252,    77],\n",
      "        [18250,  8812,  2114,  1659],\n",
      "        [  617, 34680, 27271,    13]])\n"
     ]
    }
   ],
   "source": [
    "dataloader = create_dataloader_v1(raw_text,\n",
    "                                  batch_size=8,\n",
    "                                  max_length=4,\n",
    "                                  stride=4,     # see note below\n",
    "                                  shuffle=False,\n",
    "                                  drop_last=False)\n",
    "\n",
    "data_iter = iter(dataloader)\n",
    "inputs, targets = next(data_iter)\n",
    "print(\"Inputs:\\n\", inputs)\n",
    "print(\"\\nTargets:\\n\", targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d393622e-ae6f-40ce-a3db-838fbe755f36",
   "metadata": {},
   "source": [
    "NOTE: now that we increase the stride to 4. This is to utilize the data set fully (we don't skip a single word) but also avoid any overlap between the batches, as the overlap could lead to increased overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53f79d00-fece-4bb7-993c-448322591e6c",
   "metadata": {},
   "source": [
    "# Token Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69c01d6a-a03b-406a-a5cf-2ac8adc90ab2",
   "metadata": {},
   "source": [
    "Let us see how the token ID to embedding vector conversion works with a hands-on example. We have four input tokens "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "48add4ed-b307-4be0-894b-a22344a70aac",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = torch.tensor([2,3,5,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b21a27f2-717d-4ccf-99d5-e17224cfcd35",
   "metadata": {},
   "source": [
    "for the sake of simplicity we are going to use small vocabulary of only 6 words(instead of 50,257 words in the BPE tokenizer vocabulary) and we want to create embeddings of size 3 (in GPT-3, the embedding size is 12,288 dimensions):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "dfce392d-9704-4d95-8454-caa2b4ca148a",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size=6\n",
    "output_dim = 3\n",
    "\n",
    "torch.manual_seed(123)\n",
    "embedding_layer = torch.nn.Embedding(vocab_size, output_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39a38542-65a6-4c3f-a968-c2ff06579222",
   "metadata": {},
   "source": [
    "embedding is a simple lookup table that stores embeddings of a fixed dictionary and size. and this intialize the weights of the embedding matrix in a random manner. here we will have the embedding matrix of 6 rows and 3 columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "e236b7b6-38a7-45a6-9ef9-17d215489693",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[ 0.3374, -0.1778, -0.1690],\n",
      "        [ 0.9178,  1.5810,  1.3010],\n",
      "        [ 1.2753, -0.2010, -0.1606],\n",
      "        [-0.4015,  0.9666, -1.1481],\n",
      "        [-1.1589,  0.3255, -0.6315],\n",
      "        [-2.8400, -0.7849, -1.4096]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "#the print statement in the code prints the embedding layer's underlyingg weight matrix:\n",
    "print(embedding_layer.weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4482d1e-a80b-4399-bbc5-549abcbdd9d4",
   "metadata": {},
   "source": [
    "the above are the initial weights in the embedding layer and it contains small and random values. and these are the values that are optimized during LLM training as a part of the LLM optimization itself. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "7b7a3bb5-6e76-40b3-8e3c-a2ad60e5749f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.4015,  0.9666, -1.1481]], grad_fn=<EmbeddingBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(embedding_layer(torch.tensor([3]))) #fetches the vector embedding for ID 3."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd9712a7-2451-480e-b9c7-aa61a7637fcb",
   "metadata": {},
   "source": [
    "If we look at the embedding vector for **token ID 3**, it’s exactly the same as the **4th row** in the embedding matrix (because Python counts from zero).\n",
    "\n",
    "This means that the **embedding layer** basically works like a **lookup table** — it just picks the correct row (vector) from its weight matrix based on the **token’s ID**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "ed14c349-f893-4bb4-a18b-c887b527eaef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.2753, -0.2010, -0.1606],\n",
      "        [-0.4015,  0.9666, -1.1481],\n",
      "        [-2.8400, -0.7849, -1.4096],\n",
      "        [ 0.9178,  1.5810,  1.3010]], grad_fn=<EmbeddingBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(embedding_layer(input_ids))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59aca6a2-c718-4545-9145-52eb8a99a637",
   "metadata": {},
   "source": [
    "Each row in this output matrix is obtained via a lookup operation from the embedding weight matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b0772b5-a80a-47cb-b0b9-b015902f5519",
   "metadata": {},
   "source": [
    "# Positional Embeddings (Encoding word positions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d573bdd5-25d1-481d-8d59-a8ab5519c06a",
   "metadata": {},
   "source": [
    "previously we focused on very small embedding sizes in this chapter for illustration purposes. We now consider more realistic and usefule mebedding sizes and encode the input toens into a 256-dimensional vector representations. This is smaller than what the original GPT-3 model used in (in GPT-3, the embedding size is 12,288 dimensions) but still reasonable for experimentation.\n",
    "we assume, the token IDs were created by the BPE tokenizer that we implemented earlier, which has a vocabulary size of 50,257."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "18cad937-6183-4618-b65a-f636fe46e0da",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 50257\n",
    "output_dim = 256 #vector size\n",
    "\n",
    "token_embedding_layer = torch.nn.Embedding(vocab_size,  output_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09392606-0cef-4776-86f7-bf447d506fce",
   "metadata": {},
   "source": [
    "using the token_embedding_layer above, if we sample data from the data loader, we embed each token in each batch into a 256 - dimensional vector. If we have a batch_size of 8 with four tokens each, the result will be an 8 X 4 X 256 tensor.\n",
    "\n",
    "here 8 represents the batch-size and 4 represents the context size which means that the maximum input length is 4(at most 4 tokens can be passed as input). In short here the parameters would be updated after every 8 batches.\n",
    "\n",
    "Let's instantiate the data loader ( data sampling with a sliding window), first:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "71818bdb-c2df-483e-8c02-a7708c7e6c48",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 4\n",
    "# Maximum sequence length (number of tokens per training example).\n",
    "# With next-token prediction, each input chunk will be length 4.\n",
    "\n",
    "dataloader = create_dataloader_v1(\n",
    "    raw_text,            # The full corpus (string or token IDs) to turn into batches\n",
    "    batch_size=8,        # How many sequences per batch\n",
    "    max_length=max_length,  # Truncate/window each sequence to 4 tokens\n",
    "    stride=1,   # Move the window by 4 each time → non-overlapping chunks\n",
    "    shuffle=False        # Keep chunk order deterministic (useful for debugging)\n",
    ")\n",
    "\n",
    "data_iter = iter(dataloader)\n",
    "# Create a Python iterator over the DataLoader so we can pull batches manually.\n",
    "\n",
    "inputs, targets = next(data_iter)\n",
    "# Grab the first batch.\n",
    "# inputs:  [batch_size, max_length] token IDs used as model input.\n",
    "# targets: [batch_size, max_length] token IDs shifted by one step (next-token labels).\n",
    "# (I.e., for language modeling: model predicts targets[t] from inputs[t].)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "b2e36940-c0e9-440e-be11-663dddc27c3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token IDs : \n",
      " tensor([[15496,    11,   466,   345],\n",
      "        [   11,   466,   345,   588],\n",
      "        [  466,   345,   588,  8887],\n",
      "        [  345,   588,  8887,    30],\n",
      "        [  588,  8887,    30,   220],\n",
      "        [ 8887,    30,   220, 50256],\n",
      "        [   30,   220, 50256,   554],\n",
      "        [  220, 50256,   554,   262]])\n",
      "\n",
      " Inputs shape :\n",
      " torch.Size([8, 4])\n"
     ]
    }
   ],
   "source": [
    "print(\"Token IDs : \\n\", inputs)\n",
    "print(\"\\n Inputs shape :\\n\", inputs.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba89f58c-5b42-468d-b52b-2e56dff0ec6f",
   "metadata": {},
   "source": [
    "above are the token ids for the input that we have received , now what we have to do is for each of the token ids we have to convert each of this into a 256 dimensional vector representation. I have adjusted the stride due to an error that i was encountering . \n",
    "\n",
    "\n",
    "As we can see, the token ID tensor is 8X4 dimensional, menaing that the data batch consists of 8 text samples with 4 tokens each.\n",
    "\n",
    "in the below cell we would be passing this input tokens to the embedding layer to embed these token IDs into a 256-dimensional vectors: The lookup table generates the 256 dimensional vectors for each of the token ids passed on to it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "4d6c0513-6281-4ad0-9979-44368a0fe3e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 4, 256])\n"
     ]
    }
   ],
   "source": [
    "token_embeddings = token_embedding_layer(inputs)\n",
    "print(token_embeddings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "e298941d-249c-492d-93e7-f0ad444458f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating an embedding layer for the positional encoding and based on that we will encode the token ids\n",
    "\n",
    "context_length = max_length\n",
    "pos_embedding_layer = torch.nn.Embedding(context_length, output_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "f6100f61-4fac-4a3c-bd93-f46008028756",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 256])\n"
     ]
    }
   ],
   "source": [
    "pos_embeddings = pos_embedding_layer(torch.arange(max_length))\n",
    "print(pos_embeddings.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afb3a698-2817-4a71-8eb4-1ad0bc3b793b",
   "metadata": {},
   "source": [
    "in the above line of code , the input to the pos_embeddings is usually a placeholder vector torch.arange(context_length), a sequence of numbers 0,1,.., up to the maximum input length - 1.\n",
    "\n",
    "the context_length is a variable that represents the supported input size of the LLM.\n",
    "\n",
    "Here, we choose it similar to the maximum length of the input text.\n",
    "\n",
    "In practice, input text can be longer than the supported context length, in which case we have to truncate."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba6c72a1-2e39-4cf7-be07-3d8331ce1eb5",
   "metadata": {},
   "source": [
    "As we can see that the positional embedding tensor consists of four 256-dimensional vectors. we can now add these directly to the token embeddings , where Pytorch will add the 4 X 256 dimensional pos_embeddings tensor to each 4X256 dimensional token embedding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd4c4e66-6f2a-487e-aade-bd2c34786dc8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8f7d57f-b8ca-4d35-bd10-39b3ad064cd4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
