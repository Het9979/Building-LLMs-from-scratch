{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "34f74039-89b4-4b3a-b8e6-a876f7057509",
   "metadata": {},
   "source": [
    "# Step 1: Creating Tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "99f53552-3e5e-44e8-93e4-3bcb42683871",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total number of characters:  20479\n",
      "I HAD always thought Jack Gisburn rather a cheap genius--though a good fellow enough--so it was no \n"
     ]
    }
   ],
   "source": [
    "with open(\"the-verdict.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    raw_text = f.read() #storing the content form verdict.txt file\n",
    "\n",
    "print(\"total number of characters: \", len(raw_text))\n",
    "print(raw_text[:99]) #prints the first 100 characters of the file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c151902-c9cb-4d4b-9da3-f5eb93eaeaf0",
   "metadata": {},
   "source": [
    "our goal is to convert the 20479 characters into individual words and that we can turn into embeddigs for LLM training.\n",
    "\n",
    "\n",
    "now the question is how can we best split this text to obtain a list of tokens?\n",
    "for this we will use python's regular expression library and then split the text based on the white space or punctuations into individual tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a82eb761-8847-4b4f-9362-232c3484ddc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello,', ' ', 'world.', ' ', 'This,', ' ', 'is', ' ', 'a', ' ', 'test.']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "text = \"Hello, world. This, is a test.\"\n",
    "result= re.split(r'(\\s)', text) #splits wherever whitespaces are encountered.\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc21f6c8-e2f0-48d8-ba2d-475151f4aa8d",
   "metadata": {},
   "source": [
    "from the above code we can see that the result is a list of individual words, whitespaces, and punctuations. Now lets modify the regular expression such that it splits on whitespaces (\\s) and commas, and period."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "00bf4cf1-4d6e-44a1-a4be-69cf46857f7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', ',', '', ' ', 'world', '.', '', ' ', 'This', ',', '', ' ', 'is', ' ', 'a', ' ', 'test', '.', '']\n"
     ]
    }
   ],
   "source": [
    "result = re.split(r'([,.]|\\s)', text)\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d4b4532-dcf9-46c0-86ef-da8313d58320",
   "metadata": {},
   "source": [
    " a small issue that we encounter here is that the list still includes the whitespace characters. we have to remove them , which is as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "76d1e2aa-7f43-4c26-bef7-74976bea32c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', ',', 'world', '.', 'This', ',', 'is', 'a', 'test', '.']\n"
     ]
    }
   ],
   "source": [
    "result = [item for item in result if item.strip()] #scans each item in the result and removes whitespace\n",
    "#item.strip() will only return true if there is a word or punctuation else return false and it will not print\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d05013df-d7f3-48d5-a43f-71093f005d6b",
   "metadata": {},
   "source": [
    "REMOVING WHITESPACES OR NOT?\n",
    "\n",
    "When developing a simple tokenizer , whether we should encode whitespaces as separate characters or just remove them depends on our application and its requirements. \n",
    "\n",
    "Advantages of removing white spaces is that it reduces the memory and computing requirements. However keeping them can be useful if we train models that are sensitive to the exact structure of the text. (for example , python code , which is sensitive to indentation and spacing)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfa98bed-1186-459c-b063-e7e4acccbba7",
   "metadata": {},
   "source": [
    "The tokenization scheme that we have used above is well enough but the input text can contain various other things such as question marks, quotation marks , double -dashes etc so we will again modify the splitting criteria based on the nature of this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "df7c254e-9db9-40e6-8cf1-d16d2199a08d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', ',', 'world', '!', '.', 'Is', 'this', '--', 'a', 'test', '?']\n"
     ]
    }
   ],
   "source": [
    "text = \"Hello, world!. Is this-- a test?\"\n",
    "result = re.split(r'([,.:;?_!\"()\\']|--|\\s)', text)\n",
    "\n",
    "result = [item.strip() for item in result if item.strip()]\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "18171ade-5855-4aeb-ad9a-33bd078e2860",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', ',', 'world', '!', '.', 'Is', 'this', '--', 'a', 'test', '?']\n"
     ]
    }
   ],
   "source": [
    "#strip whitespace from each item and then filter out any empty string\n",
    "\n",
    "result = [item for item in result if item.strip()]\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "eb2f8784-c64e-4138-b1d0-4a19742a172d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', ',', 'world', '!', '.', 'Is', 'this', '--', 'a', 'test', '?']\n"
     ]
    }
   ],
   "source": [
    "text = \"Hello, world!. Is this-- a test?\"\n",
    "result = re.split(r'([,.:;?_!\"()\\']|--|\\s)', text)\n",
    "\n",
    "result = [item.strip() for item in result if item.strip()] \n",
    "#the first item.strip() checks for whitespaces at the beginning of the sentence.\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9b95d4b-004f-47d5-908e-9815b8d2ba76",
   "metadata": {},
   "source": [
    "### Now let's apply this tokenizer to the our raw data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c39c0a5d-f9a5-4919-8bc0-9b15e70300b4",
   "metadata": {},
   "source": [
    "apply this tokenizer to or data and then store it to a variable named preprocessed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a89a542c-ee04-40f7-b09f-f47118727981",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'HAD', 'always', 'thought', 'Jack', 'Gisburn', 'rather', 'a', 'cheap', 'genius', '--', 'though', 'a', 'good', 'fellow', 'enough', '--', 'so', 'it', 'was', 'no', 'great', 'surprise', 'to', 'me', 'to', 'hear', 'that', ',', 'in']\n"
     ]
    }
   ],
   "source": [
    "preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', raw_text)\n",
    "preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
    "print(preprocessed[:30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "20ea2de9-f0f0-418a-83d8-8bbed8068023",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4690\n"
     ]
    }
   ],
   "source": [
    "print(len(preprocessed)) #this prints the length of the entire preprocessed token."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c467d0a9-3fb1-4e6c-a64d-853c96cb61ae",
   "metadata": {},
   "source": [
    "we have successully tokenized the entire dataset that we had and now we proceed to the second step where we assign ids to the tokens because machines cannot understand the tokens directly we have to assign IDs to the tokens."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "693c2de0-bc1d-4541-a218-68cfa1e51f9d",
   "metadata": {},
   "source": [
    "# Step 2: Creating Token IDs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43fc589e-b9a9-46e3-a795-fb65db227af8",
   "metadata": {},
   "source": [
    "in this step we will sort the tokens in the preprocessed variable in alphabetical order and then determine the vocabulary size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6d48353b-2512-45f4-b633-741aece919a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1130\n"
     ]
    }
   ],
   "source": [
    "all_words = sorted(set(preprocessed))  #converting it into a set and then sorting in the set\n",
    "vocab_size = len(all_words)\n",
    "\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56b55251-2da2-459c-a600-9e9fdd8f5f67",
   "metadata": {},
   "source": [
    "here the number is less as compared to the tokens because the vocab size is the count of only the unique toekns that are present in the preprocessed variable.\n",
    "\n",
    "\n",
    "now assigning this to vocabulary where vocabulary is like a dictionary of tokens and their associated token IDs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "20ec07da-a530-48f0-b475-03519f5517b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = {token:integer for integer, token in enumerate(all_words)}\n",
    "#this will assign integer to each and every unique token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7f9efbaa-b708-4366-903e-7e72a4d16f5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('!', 0)\n",
      "('\"', 1)\n",
      "(\"'\", 2)\n",
      "('(', 3)\n",
      "(')', 4)\n",
      "(',', 5)\n",
      "('--', 6)\n",
      "('.', 7)\n",
      "(':', 8)\n",
      "(';', 9)\n",
      "('?', 10)\n",
      "('A', 11)\n",
      "('Ah', 12)\n",
      "('Among', 13)\n",
      "('And', 14)\n",
      "('Are', 15)\n",
      "('Arrt', 16)\n",
      "('As', 17)\n",
      "('At', 18)\n",
      "('Be', 19)\n",
      "('Begin', 20)\n",
      "('Burlington', 21)\n",
      "('But', 22)\n",
      "('By', 23)\n",
      "('Carlo', 24)\n",
      "('Chicago', 25)\n",
      "('Claude', 26)\n",
      "('Come', 27)\n",
      "('Croft', 28)\n",
      "('Destroyed', 29)\n",
      "('Devonshire', 30)\n",
      "('Don', 31)\n",
      "('Dubarry', 32)\n",
      "('Emperors', 33)\n",
      "('Florence', 34)\n",
      "('For', 35)\n",
      "('Gallery', 36)\n",
      "('Gideon', 37)\n",
      "('Gisburn', 38)\n",
      "('Gisburns', 39)\n",
      "('Grafton', 40)\n",
      "('Greek', 41)\n",
      "('Grindle', 42)\n",
      "('Grindles', 43)\n",
      "('HAD', 44)\n",
      "('Had', 45)\n",
      "('Hang', 46)\n",
      "('Has', 47)\n",
      "('He', 48)\n",
      "('Her', 49)\n",
      "('Hermia', 50)\n"
     ]
    }
   ],
   "source": [
    "for i, item in enumerate(vocab.items()):\n",
    "    print(item)\n",
    "    if i>=50:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c5e2b689-4936-49aa-82e8-97494767fb0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleTokenizerV1:\n",
    "    def __init__(self, vocab):\n",
    "        self.str_to_int = vocab\n",
    "        self.int_to_str = {i:s for s, i in vocab.items()} #needed for the decoder part to convert num to token\n",
    "\n",
    "    def encode(self, text):\n",
    "        preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', text)\n",
    "        preprocessed = [\n",
    "            item.strip() for item in preprocessed if item.strip()\n",
    "        ]\n",
    "        ids = [self.str_to_int[s] for s in preprocessed] #converting tokens into token IDs.\n",
    "        return ids\n",
    "\n",
    "    def decode(self, ids):\n",
    "        text = \" \".join([self.int_to_str[i] for i in ids]) #using reverse dictionary to convert token IDs to tokens\n",
    "        #replace spaces before specified punctuations , so  that it makes a perfect sentence.\n",
    "        text = re.sub(r'\\s+([,.?!\"()\\'])', r'\\1', text)\n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "688c4212-edaa-4f30-aaa3-1c5835ab72ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 56, 2, 850, 988, 602, 533, 746, 5, 1126, 596, 5, 1, 67, 7, 38, 851, 1108, 754, 793, 7]\n"
     ]
    }
   ],
   "source": [
    "#trying th etokenizer class that we have created by taking a sub part of the dataset for testing\n",
    "tokenizer = SimpleTokenizerV1(vocab)\n",
    "\n",
    "text = \"\"\"\"It's the last he painted, you know,\"\n",
    "            Mrs. Gisburn said with pardonable pride.\"\"\"\n",
    "\n",
    "ids = tokenizer.encode(text)\n",
    "print(ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c570cdaf-c200-4ed2-9107-5c6cf5d4f3ea",
   "metadata": {},
   "source": [
    "our tokenizer has converted the tokens into token IDs now let's test our decoeder whethter it can convert the token IDs back to tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "66c0a50c-dfb0-4cf0-8120-20607b294f9e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\" It\\' s the last he painted, you know,\" Mrs. Gisburn said with pardonable pride.'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63806a9c-7dea-48ab-9686-95ce3abc99a2",
   "metadata": {},
   "source": [
    "from the above result we can see that we have successfully converted the tokens to token IDs and tokenIDs back to tokens from the subset of the training data. Now let's move further with it. What if we provide it with a sentence which is not present in the dataset?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ff254cfc-23fc-48a9-9d99-ed80b1872cb4",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'Hello'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[24], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m#testing it with some words which are not present in the already available dataset.\u001b[39;00m\n\u001b[0;32m      3\u001b[0m text \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHello , do you like tea?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m----> 4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m)\n",
      "Cell \u001b[1;32mIn[21], line 11\u001b[0m, in \u001b[0;36mSimpleTokenizerV1.encode\u001b[1;34m(self, text)\u001b[0m\n\u001b[0;32m      7\u001b[0m preprocessed \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m([,.:;?_!\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m()\u001b[39m\u001b[38;5;130;01m\\'\u001b[39;00m\u001b[38;5;124m]|--|\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124ms)\u001b[39m\u001b[38;5;124m'\u001b[39m, text)\n\u001b[0;32m      8\u001b[0m preprocessed \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m      9\u001b[0m     item\u001b[38;5;241m.\u001b[39mstrip() \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m preprocessed \u001b[38;5;28;01mif\u001b[39;00m item\u001b[38;5;241m.\u001b[39mstrip()\n\u001b[0;32m     10\u001b[0m ]\n\u001b[1;32m---> 11\u001b[0m ids \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstr_to_int\u001b[49m\u001b[43m[\u001b[49m\u001b[43ms\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m preprocessed] \u001b[38;5;66;03m#converting tokens into token IDs.\u001b[39;00m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ids\n",
      "\u001b[1;31mKeyError\u001b[0m: 'Hello'"
     ]
    }
   ],
   "source": [
    "#testing it with some words which are not present in the already available dataset.\n",
    "\n",
    "text = \"Hello , do you like tea?\"\n",
    "print(tokenizer.encode(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31a00042-4679-4934-842a-df78c879ad00",
   "metadata": {},
   "source": [
    "we get an error for the above sentence because we don't have the word Hello in our dataset and from this we get to know that we need to consider large and diverse training sets to exxtend the vocabulary when working on LLMs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f25c2d02-7254-4a90-8776-803622ddc1b3",
   "metadata": {},
   "source": [
    "# Adding Special Context Tokens\n",
    "\n",
    "in the previous section we have implemented a simple tokenizer and which when tested to tokenize a word which was not present in the trainig data it gave an error. So in this section we will modify the tokenizer to handle unknown words. I particular, we will modify the vocabulary and tokenizer we implemented in the previous section, here we will implement the version 2 of SimpleTokenizer to handle the unknown tokens."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce4f411e-024b-41e5-a4b1-e873cc6226df",
   "metadata": {},
   "source": [
    "we can modify the tokenizer to use an <|unk|> token if it encounters a word that is not a part of the voocabulary. Furhtermore we add a token between unrelated tasks. \n",
    "For example , when training GPT-like LLMs multiple independent documents or books, it is common to insert a token before each document or book that follows a previous text source.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f1c3eb9-72c9-4092-9644-1ef6d3797722",
   "metadata": {},
   "source": [
    "modifying the vocabulary to include these two special tokens , <|endoftext|> and <|unk|> to the existing vocabulary. Previously the size of the vocabulary was 1130 and after addding this two tokens it would increase and become 1132."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ed21d709-45f7-4052-925d-75707ce3fc5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_tokens = sorted(list(set(preprocessed)))\n",
    "all_tokens.extend([\"<|endoftext|>\", \"<|unk|>\"])\n",
    "\n",
    "vocab = {token:integer for integer, token in enumerate(all_tokens)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d6cf4112-d095-4411-a515-55f6180ae392",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1132"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3d8af38e-6033-41b7-9041-09aef54a8e2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('younger', 1127)\n",
      "('your', 1128)\n",
      "('yourself', 1129)\n",
      "('<|endoftext|>', 1130)\n",
      "('<|unk|>', 1131)\n"
     ]
    }
   ],
   "source": [
    "#for checking our modification we are printing the last 5 entries of the vocabulary.\n",
    "for i, item in enumerate(list(vocab.items())[-5:]):\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "2e469ae8-4847-4c95-a1bd-7a80566b30ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "#now further we will extend the simple tokenizer class with this.\n",
    "\n",
    "class SimpleTokenizerV2:\n",
    "    def __init__(self, vocab):\n",
    "        self.str_to_int = vocab\n",
    "        self.int_to_str = {i:s for s, i in vocab.items()}\n",
    "\n",
    "    def encode(self, text):\n",
    "        preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', text)\n",
    "        preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
    "        preprocessed = [\n",
    "            item if item in self.str_to_int\n",
    "            else \"<|unk|>\" for item in preprocessed #scans the entire text and if comes across unknown word \n",
    "        ]\n",
    "        ids = [self.str_to_int[s] for s in preprocessed]\n",
    "        return ids\n",
    "\n",
    "    def decode(self, ids):\n",
    "        text = \" \".join([self.int_to_str[i] for i in ids])\n",
    "        #replace spaces before the specified punctuations\n",
    "        text = re.sub(r'\\s+([,.:;?!\"()\\'])', r'\\1', text)\n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c77be98a-d0da-49ec-b30b-565685db0f96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, do you like tea?<|endoftext|>In the sunlit terraces of the palace.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = SimpleTokenizerV2(vocab)\n",
    "\n",
    "text1= \"Hello, do you like tea?\"\n",
    "text2= \"In the sunlit terraces of the palace.\"\n",
    "\n",
    "text = \"<|endoftext|>\".join((text1, text2))\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "8965f1fb-91a9-4717-9dff-1f5b0d25bb41",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1131, 5, 355, 1126, 628, 975, 10, 1131, 988, 956, 984, 722, 988, 1131, 7]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c026367-3172-4934-9c7b-0196de96443d",
   "metadata": {},
   "source": [
    "from the above results we can see that since hello was not present in our vocabulary it printed the token id of unk and for the endoftext also it did the same thing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "400f846a-3553-4141-8967-fec17c8fdfa4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|unk|>, do you like tea? <|unk|> the sunlit terraces of the <|unk|>.'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(tokenizer.encode(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "892078b6-51ed-47c4-9ffe-bc08c3719238",
   "metadata": {},
   "source": [
    "based on the above detokenized words we can see that two words \"Hello\" and \"palace\" were not present in our vocabulary it were replaced with <|unk|> in the decoder part."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6ffd686-d75a-48fa-a659-273f29f60ee3",
   "metadata": {},
   "source": [
    "apart from this two tokens we also have other tokens which people use like \n",
    "1. [BOS] beginning of sequence - it marks the starting of the text. it signifies to the LLM where a piece of content begins.\n",
    "2. [EOS] end of sequence - it is positioned at the end of the text and is useful when concatenating multiple unrelated text.\n",
    "3. [PAD] padding - when training LLMs with batch sizes larger than one, the batch might contain texts of varying length . To ensure all texts have the same length the shorter texts are padded using the [PAD] token , up to the length of the longest text in the batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f754d92-14a9-45ac-b9f8-a7e274f8f44c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
